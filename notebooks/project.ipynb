{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List\n",
    "\n",
    "from optimcourse.gradient_descent import gradient_descent\n",
    "from optimcourse.optim_utilities import print_rec\n",
    "from optimcourse.forward_propagation import (\n",
    "    forward_propagation, \n",
    "    create_weights, \n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from optimcourse.activation_functions import (\n",
    "    relu,\n",
    "    sigmoid\n",
    ") \n",
    "from optimcourse.test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1\n",
    ")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {},
   "source": [
    "# <center>Optimization Project</center>\n",
    "\n",
    "This notebook contains the questions of the practical session along with complementary guidelines and examples. The code is written in Python. The questions are in red and numbered from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268748c",
   "metadata": {},
   "source": [
    "## Code demo\n",
    "\n",
    "Seat and relax, we will show you how to use the code for optimizing functions.\n",
    "First plot examples of 2D functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af33f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = rosen\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.gca( projection='3d')\n",
    "#axis.set_zlim(0,150)\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n",
    "# figure.savefig('plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3560d",
   "metadata": {},
   "source": [
    "Now carry out some optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67778975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# function definition\n",
    "fun = sphereL1\n",
    "dim = 10\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "# np.random.seed(123) # useful for repeated runs (quadratic fct or initial random point)\n",
    "\n",
    "#########################\n",
    "# algorithms settings\n",
    "# start_x = np.array([3,2,1,-4.5,4.6,-2,-1,4.9,0,2])\n",
    "# start_x = (1+np.arange(dim))*5/dim\n",
    "# start_x = np.array([2.3,4.5])\n",
    "start_x = np.random.uniform(low=LB,high=UB)\n",
    "\n",
    "budget = 1000*(dim+1)\n",
    "printlevel = 1  # =0,1,2 , careful with 2 which is memory consuming\n",
    "\n",
    "#########################\n",
    "# optimize\n",
    "# res = random_opt(func=fun, LB=LB, UB=UB, budget=budget, printlevel=printlevel)\n",
    "res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,\n",
    "                       budget=budget,\n",
    "                       step_factor=0.1,direction_type=\"momentum\",\n",
    "                       do_linesearch=True,min_step_size=1e-11,\n",
    "                       min_grad_size=1e-6,inertia=0.9,\n",
    "                       printlevel=printlevel)\n",
    "\n",
    "#########################\n",
    "# reporting\n",
    "print_rec(res=res, fun=fun, dim=dim, LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e9c6c",
   "metadata": {},
   "source": [
    "## Understanding the code through an example\n",
    "\n",
    "Let us consider the following test function which is associated to machine learning :\n",
    "\n",
    "$$ f(x) = \\sum_{i=1}^n (x_i - c_i)^2 + \\lambda \\sum_{i=1}^n \\lvert x_i\\rvert \\quad,\\quad \\lambda \\ge 0 $$\n",
    "$$ c_i = i \\quad \\text{ and } \\quad -5 = LB_i \\le x_i \\le UB_i = 5 \\quad,\\quad i=1,\\ldots,n $$  \n",
    "\n",
    "* First term: sphere function centered at $c$. A simplistic model to the mean square error of a NN where $c$ minimizes the training error.\n",
    "* Second term: L1 norm times $\\lambda$. The $x_i$'s would be the weights of a NN.\n",
    "This term helps in improving the test error.\n",
    "\n",
    "The function is already coded in `test_functions.py` as `sphereL1`. $\\lambda$ is set in the function (open the file in your preferred Python editor).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let us first plot the function in 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = sphereL1\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.gca( projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a2e4f",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Questions : optimizing the `sphereL1` function </span>\n",
    "\n",
    "You will optimize the `sphereL1` function for various values of $\\lambda$, $\\lambda = \\{0.001,0.1,1,5,10\\}$ in `dim=10` dimensions.\n",
    "\n",
    "To do this, edit the `main_optim.py` file, which gives an example with the code provided, and make sure that the function is described as follows  \n",
    "```\n",
    "# function definition\n",
    "fun = test_functions.sphereL1\n",
    "dim = 10\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "\n",
    "```\n",
    "\n",
    "Repeat optimizations for varying $\\lambda$'s (parameter `lbda` in `test_functions.sphereL1`)\n",
    "The results can be put in a table as below : \n",
    "  \n",
    "| lbda  | $x^\\star$  | $f(x^\\star)$  |  \n",
    "| :---:  | :---:  | :----:  |  \n",
    "|  value 1  |  your values  |  your value  |  \n",
    "|  value 2  |  your values  |  your value  |  \n",
    "\n",
    "\n",
    "1. <span style=\"color:red\">What do you notice ?</span>  \n",
    "2. <span style=\"color:red\">Assuming the $x$'s are weights of a neural network, what would be the effect of $\\lambda$ on the network ?</span>  \n",
    "\n",
    "\n",
    "Note : when changing `lbda`, it is important to reload the kernel or, to make it automatic, add the following lines of code\n",
    "```\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979a431",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# A NEURAL NETWORK FROM SCRATCH\n",
    "\n",
    "You are encouraged to have a look at `forward_propagation`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf1731",
   "metadata": {},
   "source": [
    "### Data structure behind the forward propagation\n",
    "\n",
    "The following network has 2 layers, the first going from 4 input components to the 3 internal neurons, the second going from the 3 internal neurons outputs to the 2 outputs. Don't forget the additional weight for the neurons biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1,2,5,4],[1,0.2,0.15,0.024]])\n",
    "weights = [\n",
    "        np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1,-1],\n",
    "                [2,1,3,5,0],\n",
    "                [0.2,0.1,0.6,0.78,1]\n",
    "            ]\n",
    "        ),\n",
    "    np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1],\n",
    "                [2,1,3,5]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "activation = sigmoid\n",
    "forward_propagation(inputs,weights,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85c50e",
   "metadata": {},
   "source": [
    "### Create a data set \n",
    "The data set is made of points sampled randomly from a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_target(fun: Callable,\n",
    "                       n_features: int,\n",
    "                       n_obs: int,\n",
    "                       LB: List[float],\n",
    "                       UB: List[float]) -> dict:\n",
    "    \n",
    "    entry_data = np.random.uniform(low= LB,high=UB,\n",
    "                                   size=(n_obs, n_features))\n",
    "    target = np.apply_along_axis(fun, 1, entry_data)\n",
    "    \n",
    "    return {\"data\": entry_data, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a46295",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = linear_function\n",
    "n_features = 2\n",
    "n_obs = 10\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,\n",
    "                                      n_features = n_features,\n",
    "                                      n_obs=n_obs,LB=LB,UB=UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8496e",
   "metadata": {},
   "source": [
    "simulated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa3f58",
   "metadata": {},
   "source": [
    "### Make a neural network, randomly initialize its weights, propagate input data\n",
    "\n",
    "Create a NN with 1 layer, 2 inputs and 1 output. Propagate the data inputs through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = [2,1]\n",
    "weights = create_weights(network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector)\n",
    "print(\"weights=\",weights)\n",
    "print(\"dim=\",dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = forward_propagation(simulated_data[\"data\"],weights,sigmoid)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13714546",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bde3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311344e9",
   "metadata": {},
   "source": [
    "### Error functions \n",
    "\n",
    "A utility function to transform a vector into weight matrices. You will probably not need it, but this is used in the calculation of the error function (the vector is transformed into NN weights, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de88a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_weights([0.28677805, -0.07982693,  0.37394315],network_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b11c8f",
   "metadata": {},
   "source": [
    "We define 2 error functions, one for regression is the mean square error, the other is the cross-entropy error for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d120d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e981770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "\n",
    "    n = len(y_observed)\n",
    "    \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))\n",
    "    \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: List[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,\n",
    "                                used_network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,\n",
    "                                           activation_function)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + \\\n",
    "    regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_network_structure = [2,1] # 2 inputs features, 1 layer with 1 node\n",
    "used_activation = relu\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b31b48",
   "metadata": {},
   "source": [
    "Below, the cost function associated to the neural network is calculated from a simple vector in a manner similar to $f(x)$, therefore prone to optimization. The translation of the vector into as many weight matrices as necessary is done thanks to the `used_network_structure` defined above and passed implicitely thanks to Python's scoping rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights_as_vect = np.random.uniform(size=dim)\n",
    "neural_network_cost(random_weights_as_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb4bee",
   "metadata": {},
   "source": [
    "### Learn the network by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-5] * 3 \n",
    "UB = [5] * 3\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = np.array([0.28677805, -0.07982693,  0.37394315]),\n",
    "                 LB = LB, UB = UB,budget = 1000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dddf4",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Question : Make your own network</span>\n",
    "\n",
    "3. <span style=\"color:red\">Generate 100 data points with the quadratic function in 2 dimensions.</span>\n",
    "4. <span style=\"color:red\">Create a network with 2 inputs, 5 ReLU neurons in the hidden layer, and 1 output.</span>\n",
    "5. <span style=\"color:red\">Learn it on the quadratic data points you generated. Plot some results, discuss them.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11600427",
   "metadata": {},
   "source": [
    "# **FIN DU NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
