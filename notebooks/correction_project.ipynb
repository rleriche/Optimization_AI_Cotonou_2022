{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dcb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List\n",
    "\n",
    "from optimcourse.gradient_descent import gradient_descent\n",
    "from optimcourse.optim_utilities import print_rec\n",
    "from optimcourse.forward_propagation import (\n",
    "    forward_propagation, \n",
    "    create_weights, \n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from optimcourse.activation_functions import (\n",
    "    relu,\n",
    "    sigmoid\n",
    ") \n",
    "from optimcourse.test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {},
   "source": [
    "# <center>Optimization Project with corrections </center>\n",
    "\n",
    "This notebook contains the questions of the practical session along with complementary guidelines and examples. The code is written in Python. The questions are in red and numbered from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcab2d",
   "metadata": {},
   "source": [
    "## Code demo\n",
    "\n",
    "Seat and relax, we will show you how to use the code for optimizing functions.\n",
    "First plot examples of 2D functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = quadratic\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.gca( projection='3d')\n",
    "axis.set_zlim(0,150)\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n",
    "# figure.savefig('plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f870b",
   "metadata": {},
   "source": [
    "Now carry out some optimizations.  \n",
    "\n",
    "Some explanations about results format parameters :  \n",
    "\n",
    "printlevel : int, controls how much is recorded during optimization.  \n",
    "       &emsp; = 0 for minimum recording (best point found and its obj function value)  \n",
    "       &emsp; > 0 records history of best points  \n",
    "       &emsp; > 1 records the entire history of points (memory consuming)    \n",
    "        \n",
    "The optimization results are dictionaries with the following key-value pairs:  \n",
    "       &emsp; \"f_best\", float : best ojective function found during the search  \n",
    "       &emsp; \"x_best\", 1D array : best point found   \n",
    "       &emsp; \"stop_condition\" : str describing why the search stopped  \n",
    "       &emsp; \"time_used\" , int : time actually used by search (may be smaller than max budget)  \n",
    "       &emsp; if printlevel > 0 :  \n",
    "          &emsp;&emsp;  \"hist_f_best\", list(float) : history of best so far objective functions  \n",
    "           &emsp;&emsp; \"hist_time_best\", list(int) : times of recordings of new best so far  \n",
    "           &emsp;&emsp; \"hist_x_best\", 2D array : history of best so far points as a matrix, each x is a row  \n",
    "        &emsp;if printlevel > 1 :  \n",
    "       &emsp;&emsp; \"hist_f\", list(float) : all f's calculated  \n",
    "       &emsp;&emsp; \"hist_x\", 2D array : all x's calculated  \n",
    "       &emsp;&emsp; \"hist_time\", list(int) : times of recording of full history  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe401c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# function definition\n",
    "fun = rosen\n",
    "dim = 2\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "# np.random.seed(123) # useful for repeated runs (quadratic fct or initial random point)\n",
    "\n",
    "#########################\n",
    "# algorithms settings\n",
    "# start_x = np.array([3,2,1,-4.5,4.6,-2,-1,4.9,0,2])\n",
    "# start_x = (1+np.arange(dim))*5/dim\n",
    "# start_x = np.array([2.3,4.5])\n",
    "start_x = np.random.uniform(low=LB,high=UB)\n",
    "\n",
    "budget = 1000*(dim+1)\n",
    "printlevel = 1  # =0,1,2 , careful with 2 which is memory consuming\n",
    "\n",
    "#########################\n",
    "# optimize\n",
    "# res = random_opt(func=fun, LB=LB, UB=UB, budget=budget, printlevel=printlevel)\n",
    "res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,budget=budget,\n",
    "                       step_factor=0.1,direction_type=\"momentum\",\n",
    "                       do_linesearch=True,min_step_size=1e-11,\n",
    "                       min_grad_size=1e-6,inertia=0.9,printlevel=printlevel)\n",
    "\n",
    "#########################\n",
    "# reporting\n",
    "print_rec(res=res, fun=fun, dim=dim, LB=LB, UB=UB , printlevel=printlevel, logscale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e9c6c",
   "metadata": {},
   "source": [
    "## Understanding the code through an example\n",
    "\n",
    "Let us consider the following test function which is associated to machine learning :\n",
    "\n",
    "$$ f(x) = \\sum_{i=1}^n (x_i - c_i)^2 + \\lambda \\sum_{i=1}^n \\lvert x_i\\rvert \\quad,\\quad \\lambda \\ge 0 $$\n",
    "$$ c_i = i \\quad \\text{ and } \\quad -5 = LB_i \\le x_i \\le UB_i = 5 \\quad,\\quad i=1,\\ldots,n $$  \n",
    "\n",
    "* First term: sphere function centered at $c$. A simplistic model to the mean square error of a NN where $c$ minimizes the training error.\n",
    "* Second term: L1 norm times $\\lambda$. The $x_i$'s would be the weights of a NN.\n",
    "This term helps in improving the test error.\n",
    "\n",
    "The function is already coded in `test_functions.py` as `sphereL1`. $\\lambda$ is set in the function (open the file in your preferred Python editor).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let us first plot the function in 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = sphereL1\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.gca( projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a2e4f",
   "metadata": {},
   "source": [
    "### Questions : optimizing the `sphereL1` function\n",
    "\n",
    "You will optimize the `sphereL1` function for various values of $\\lambda$, $\\lambda = \\{0.001,0.1,1,5,10\\}$ in `dim=10` dimensions.\n",
    "\n",
    "To do this, edit the `main_optim.py` file, which gives an example with the code provided, and make sure that the function is described as follows  \n",
    "```\n",
    "# function definition\n",
    "fun = test_functions.sphereL1\n",
    "dim = 10\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "\n",
    "```\n",
    "\n",
    "Repeat optimizations for varying $\\lambda$'s (parameter `lbda` dans `test_functions.sphereL1`)\n",
    "1. What do you notice ? \n",
    "2. Assuming the $x$'s are weights of a neural network, what would be the effect of $\\lambda$ on the network ?\n",
    "\n",
    "Note : when changing `lbda`, it is important to reload the kernel or, to make it automatic, add the following lines of code\n",
    "```\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797e5e8",
   "metadata": {},
   "source": [
    "### Corrections : optimizing the `sphereL1` function\n",
    "\n",
    "Edit `test_functions.sphereL1` for changing `lbda` before executing the follow. The notebook kernel __must be restarted__ for the new `lbda` to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# function definition\n",
    "fun = sphereL1\n",
    "dim = 10\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "# np.random.seed(123) # useful for repeated runs (quadratic fct or initial random point)\n",
    "\n",
    "#########################\n",
    "# algorithms settings\n",
    "start_x = np.random.uniform(low=LB,high=UB)\n",
    "budget = 1000*(dim+1)\n",
    "printlevel = 1  # =0,1,2 , careful with 2 which is memory consuming\n",
    "\n",
    "#########################\n",
    "# optimize\n",
    "# res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,budget=budget,printlevel=printlevel)\n",
    "res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,budget=budget,\n",
    "                       step_factor=0.1,direction_type=\"momentum\",\n",
    "                       do_linesearch=True,min_step_size=1e-11,\n",
    "                       min_grad_size=1e-6,inertia=0.9,printlevel=printlevel)\n",
    "# reporting\n",
    "print(f'search stopped after {res[\"time_used\"]} evaluations of f because of {res[\"stop_condition\"]}')\n",
    "print(\"best objective function =\",res[\"f_best\"])\n",
    "print(\"best x =\", res[\"x_best\"])\n",
    "if printlevel > 0:\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    plt.yscale(\"log\")\n",
    "    ax1.plot((res[\"hist_time_best\"]+ [res[\"time_used\"]]) , (res[\"hist_f_best\"] + [res[\"f_best\"]]))\n",
    "    ax1.set_xlabel(\"no. calls to f\")\n",
    "    ax1.set_ylabel(\"f\")\n",
    "    if printlevel > 1:\n",
    "        ax1.plot(res[\"hist_time\"],res[\"hist_f\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5859b",
   "metadata": {},
   "source": [
    "Results : \n",
    "  \n",
    "| lbda  | $x^\\star$  | $f(x^\\star)$  |  \n",
    "| :---:  | :---:  | :----:  |  \n",
    "|  0.01  |  $0.99, 1.99, 2.99, 3.99, 4.99, 5., 5., 5., 5., 5.$  |  55.40  |  \n",
    "|  0.1  |  $0.95, 1.95, 2.95, 3.95, 4.95, 5., 5., 5., 5., 5.$  |  58.99  |  \n",
    "|  1  |  $0.5, 1.5, 2.5, 3.5, 4.5, 5., 5., 5., 5., 5.$  |  93.75  |  \n",
    "|  3  |  $0., 0.39, 1.46, 2.46, 3.56, 4.58, 5., 5., 5., 5.$  |  163.77  |  \n",
    "|  6  |  $ 0., 0.01, 0.42, 1.33, 1.84, 3.16, 4.01, 4.63, 5., 5.$  |  250.48  |  \n",
    "|  10  |  $ 0, 0, 0.05, 0.26, 0.47, 1.25, 1.73, 3.38, 3.67, 5.$  |  331.38  |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d5731",
   "metadata": {},
   "source": [
    "__Question 1__ :  \n",
    "    \n",
    "As $\\lambda$ increases, $x^\\star$ moves away from $c$ and tends to 0. Some components of $x^\\star$, those related to the low component values of $c$, are set to 0 faster than the others.  \n",
    "This can be understood by looking at an optimization problem with a constraint on the L1 norm of $x$,\n",
    "\\begin{equation*}\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "\\min_x f(x) = \\lVert x - c \\rVert^2 \\\\\n",
    "\\text{tel que }~~ g(x) = \\lVert x \\rVert_1 - \\tau \\le 0 \\quad,\\quad \\tau>0\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation*}\n",
    "The associated Lagrangian, to be minimized on $x$, is\n",
    "\\begin{equation*}\n",
    "\\min_x f(x) + \\lambda^\\star g(x) = \\lVert x - c \\rVert^2 + \\lambda^\\star \\lVert x \\rVert_1 - \\lambda^\\star\\tau\n",
    "\\end{equation*}\n",
    "\n",
    "The last term does not depend on $x$, and the 2 other terms are precisely those of the `sphereL1` function.\n",
    "The drawing below shows the sphere function and the limit of the constraint on $\\lVert x \\rVert_1$. \n",
    "It is observed that the solution tends to be at a vertex of the feasible domain where components in $x$ cancel out.\n",
    "This phenomenon becomes more visible when dimension increases.  \n",
    "\n",
    "<img src=\"l1sphere_regularization-crop.png\" alt=\"L1regularization\" width=\"300\"/>\n",
    "\n",
    "__Question 2__ :  \n",
    "\n",
    "Analogy with machine learning : if the components of $x$ are neural net weights, neuron connexions are deleted when some $x_i$'s are zero. This will prevent the network from overfitting the data. Generalization will be better. An important choice is the value of $\\lambda$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71080f0e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# A NEURAL NETWORK FROM SCRATCH\n",
    "\n",
    "First let's import the needed modules. You are encouraged to have a look at `forward_propagation`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b875f8",
   "metadata": {},
   "source": [
    "### Data structure behind the forward propagation\n",
    "\n",
    "The following network has 2 layers, the first going from 4 input components to the 3 internal neurons, the second going from the 3 internal neurons outputs to the 2 outputs. Don't forget the additional weight for the neurons biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1,2,5,4],[1,0.2,0.15,0.024]])\n",
    "weights = [\n",
    "        np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1,-1],\n",
    "                [2,1,3,5,0],\n",
    "                [0.2,0.1,0.6,0.78,1]\n",
    "            ]\n",
    "        ),\n",
    "    np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1],\n",
    "                [2,1,3,5]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "activation = sigmoid\n",
    "forward_propagation(inputs,weights,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb1fa2",
   "metadata": {},
   "source": [
    "### Create a data set \n",
    "The data set is made of points sampled randomly from a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_target(fun: Callable,\n",
    "                       n_features: int,\n",
    "                       n_obs: int,\n",
    "                       LB: list[float],\n",
    "                       UB: list[float]) -> dict:\n",
    "    \n",
    "    entry_data = np.random.uniform(low= LB,high=UB,\n",
    "                                   size=(n_obs, n_features))\n",
    "    target = np.apply_along_axis(fun, 1, entry_data)\n",
    "    \n",
    "    return {\"data\": entry_data, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014047",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = linear_function\n",
    "n_features = 2\n",
    "n_obs = 10\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,n_features = n_features,n_obs=n_obs,LB=LB,UB=UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3624c",
   "metadata": {},
   "source": [
    "### Make a neural network, randomly initialize its weights, propagate input data\n",
    "\n",
    "Create a NN with 1 layer, 2 inputs and 1 output. Propagate the data inputs through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = [2,1]\n",
    "weights = create_weights(network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector)\n",
    "print(\"weights=\",weights)\n",
    "print(\"dim=\",dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = forward_propagation(simulated_data[\"data\"],weights,sigmoid)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2afa19",
   "metadata": {},
   "source": [
    "Compare the data and the prediction of the network. Of course, at this point, no training is done so they are different. They just have the same format (provided a `reshape` is done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea2bb7",
   "metadata": {},
   "source": [
    "### Error functions \n",
    "\n",
    "A utility function to transform a vector into weight matrices. You will probably not need it, but this is used in the calculation of the error function (the vector is transformed into NN weights, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_weights([0.28677805, -0.07982693,  0.37394315],network_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c1487",
   "metadata": {},
   "source": [
    "We define 2 error functions, one for regression is the mean square error, the other is the cross-entropy error for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855acb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "\n",
    "    n = len(y_observed)\n",
    "    \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))\n",
    "    \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: list[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,used_network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,activation_function)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_network_structure = [2,1] # 2 inputs features, 1 layer with 1 node\n",
    "used_activation = relu\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0379da",
   "metadata": {},
   "source": [
    "Below, the cost function associated to the neural network is calculated from a simple vector in a manner similar to $f(x)$, therefore prone to optimization. The translation of the vector into as many weight matrices as necessary is done thanks to the `used_network_structure` defined above and passed implicitely thanks to Python's scoping rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights_as_vect = np.random.uniform(size=dim)\n",
    "neural_network_cost(random_weights_as_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddbb2a",
   "metadata": {},
   "source": [
    "### Learn the network by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443aa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-5] * 3 \n",
    "UB = [5] * 3\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = np.array([0.28677805, -0.07982693,  0.37394315]),\n",
    "                 LB = LB, UB = UB,budget = 1000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9627",
   "metadata": {},
   "source": [
    "## Question : Make your own network\n",
    "\n",
    "3. Generate 100 data points with the quadratic function in 2 dimensions.\n",
    "4. Create a network with 2 inputs, 5 ReLU neurons in the hidden layer, and 1 output.\n",
    "5. Learn it on the quadratic data points you generated. Plot some results, discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28abda",
   "metadata": {},
   "source": [
    "### Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79166f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = quadratic\n",
    "n_features = 2\n",
    "n_obs = 100\n",
    "LBfeatures = [-5] * n_features\n",
    "UBfeatures = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,n_features = n_features,n_obs=n_obs,\n",
    "                                      LB=LBfeatures,UB=UBfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f76f3",
   "metadata": {},
   "source": [
    "### Create the network\n",
    "and calculate the cost function of the first, randomly initialized, network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d145dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = [2,5,1]\n",
    "weights = create_weights(network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector) \n",
    "used_network_structure = [2,5,1]\n",
    "used_activation = relu\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "print(\"Number of weights to learn : \",dim)\n",
    "print(\"Initial cost of the NN : \",neural_network_cost(weights_as_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fb300",
   "metadata": {},
   "source": [
    "### Learn the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-10] * dim\n",
    "UB = [10] * dim\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = weights_as_vector,\n",
    "                 LB = LB, UB = UB,budget = 10000,printlevel=printlevel)\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n",
    "\n",
    "weights_best = vector_to_weights(res[\"x_best\"],network_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best NN weights:\",weights_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd42bc",
   "metadata": {},
   "source": [
    "Compare the network prediction to the true function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f334d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# function definition\n",
    "dimFunc = 2\n",
    "LBfunc = [-5,-5]\n",
    "UBfunc = [5,5]\n",
    "fun = quadratic\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "x1 = np.linspace(start=LBfunc[0], stop=UBfunc[0],num=no_grid)\n",
    "x2 = np.linspace(start=LBfunc[1], stop=UBfunc[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "# work in progress\n",
    "# does not work yet\n",
    "# zNN = np.apply_along_axis(forward_propagation,0,xy,weights_best,used_activation)\n",
    "figure = plt.figure()\n",
    "axis = figure.gca( projection='3d')\n",
    "axis.set_zlim(0,100)\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0f185",
   "metadata": {},
   "source": [
    "# **FIN DU NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
